#!/usr/bin/env python3
"""
å›¾ç‰‡è¯†åˆ«å·¥å…· - v0.6.0 Phase 4

æ”¯æŒå›¾ç‰‡ä¸Šä¼ ã€åˆ†æå’Œç†è§£
ä½¿ç”¨Claude Visionæˆ–GPT-4Vè¿›è¡Œå›¾ç‰‡è¯†åˆ«
"""

import os
import base64
import requests
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()


class VisionTool:
    """å›¾ç‰‡è¯†åˆ«å·¥å…·ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–è§†è§‰å·¥å…·"""
        self.api_type = os.getenv("AI_API_TYPE", "deepseek")
        self.claude_key = os.getenv("CLAUDE_API_KEY")
        self.openai_key = os.getenv("OPENAI_API_KEY")
        self.qwen_key = os.getenv("QWEN_API_KEY")

        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        self.supported_formats = {'.jpg', '.jpeg',
                                  '.png', '.gif', '.webp', '.bmp'}

        # ä¸Šä¼ ç›®å½•
        self.upload_dir = Path("uploads")
        self.upload_dir.mkdir(exist_ok=True)

    def encode_image(self, image_path: str) -> str:
        """
        å°†å›¾ç‰‡ç¼–ç ä¸ºbase64å­—ç¬¦ä¸²

        Args:
            image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„

        Returns:
            str: base64ç¼–ç çš„å›¾ç‰‡æ•°æ®
        """
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    def validate_image(self, image_path: str) -> tuple[bool, str]:
        """
        éªŒè¯å›¾ç‰‡æ–‡ä»¶

        Args:
            image_path: å›¾ç‰‡è·¯å¾„

        Returns:
            tuple: (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
        """
        path = Path(image_path)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not path.exists():
            return False, f"æ–‡ä»¶ä¸å­˜åœ¨: {image_path}"

        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        if path.suffix.lower() not in self.supported_formats:
            return False, f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(self.supported_formats)}"

        # æ£€æŸ¥æ–‡ä»¶å¤§å° (é™åˆ¶20MB)
        max_size = 20 * 1024 * 1024
        if path.stat().st_size > max_size:
            return False, f"æ–‡ä»¶è¿‡å¤§: {path.stat().st_size / 1024 / 1024:.1f}MB (æœ€å¤§20MB)"

        return True, ""

    def analyze_with_qwen(self, image_path: str, prompt: str = "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡") -> Dict[str, Any]:
        """ä½¿ç”¨é€šä¹‰åƒé—® Qwen-VL åˆ†æå›¾ç‰‡"""
        if not self.qwen_key:
            return {'success': False, 'error': 'Qwen APIå¯†é’¥æœªé…ç½®'}

        valid, error = self.validate_image(image_path)
        if not valid:
            return {'success': False, 'error': error}

        base64_image = self.encode_image(image_path)
        image_format = Path(image_path).suffix[1:]
        if image_format == 'jpg':
            image_format = 'jpeg'

        try:
            url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation"
            headers = {
                "Authorization": f"Bearer {self.qwen_key}",
                "Content-Type": "application/json"
            }

            data = {
                "model": "qwen-vl-max",  # ä½¿ç”¨ max ç‰ˆæœ¬ï¼Œè¯†åˆ«æ›´å‡†ç¡®
                "input": {
                    "messages": [{
                        "role": "user",
                        "content": [
                            {"image": f"data:image/{image_format};base64,{base64_image}"},
                            {"text": prompt}
                        ]
                    }]
                },
                "parameters": {}
            }

            response = requests.post(
                url, headers=headers, json=data, timeout=30)
            response.raise_for_status()
            result = response.json()

            if result.get('output') and result['output'].get('choices'):
                description = result['output']['choices'][0]['message']['content'][0]['text']
                return {
                    'success': True,
                    'description': description,
                    'model': 'qwen-vl-max',
                    'timestamp': datetime.now().isoformat()
                }
            else:
                return {'success': False, 'error': f'æ— æ³•è§£æAPIå“åº”: {result}'}

        except requests.exceptions.HTTPError as e:
            resp_text = ''
            try:
                resp_text = e.response.text
            except Exception:
                pass
            return {
                'success': False,
                'error': f'APIè¯·æ±‚å¤±è´¥: {str(e)}',
                'details': resp_text
            }
        except Exception as e:
            return {'success': False, 'error': f'åˆ†æå¤±è´¥: {str(e)}'}

    def analyze_with_claude(self, image_path: str, prompt: str = "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹") -> Dict[str, Any]:
        """
        ä½¿ç”¨Claude Visionåˆ†æå›¾ç‰‡

        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            prompt: åˆ†ææç¤ºè¯­

        Returns:
            dict: åˆ†æç»“æœ
        """
        if not self.claude_key:
            return {
                'success': False,
                'error': 'Claude APIå¯†é’¥æœªé…ç½®'
            }

        # éªŒè¯å›¾ç‰‡
        valid, error = self.validate_image(image_path)
        if not valid:
            return {'success': False, 'error': error}

        # ç¼–ç å›¾ç‰‡
        base64_image = self.encode_image(image_path)

        # è·å–å›¾ç‰‡æ ¼å¼
        image_format = Path(image_path).suffix[1:]  # å»æ‰ç‚¹å·
        if image_format == 'jpg':
            image_format = 'jpeg'

        try:
            # è°ƒç”¨Claude API
            url = "https://api.anthropic.com/v1/messages"
            headers = {
                "x-api-key": self.claude_key,
                "anthropic-version": "2023-06-01",
                "content-type": "application/json"
            }

            data = {
                "model": "claude-3-5-sonnet-20241022",
                "max_tokens": 1024,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": f"image/{image_format}",
                                    "data": base64_image
                                }
                            },
                            {
                                "type": "text",
                                "text": prompt
                            }
                        ]
                    }
                ]
            }

            response = requests.post(
                url, headers=headers, json=data, timeout=30)
            response.raise_for_status()

            result = response.json()

            return {
                'success': True,
                'description': result['content'][0]['text'],
                'model': 'claude-3.5-sonnet',
                'timestamp': datetime.now().isoformat()
            }

        except requests.exceptions.RequestException as e:
            return {
                'success': False,
                'error': f'APIè¯·æ±‚å¤±è´¥: {str(e)}'
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'åˆ†æå¤±è´¥: {str(e)}'
            }

    def analyze_with_gpt4v(self, image_path: str, prompt: str = "What's in this image?") -> Dict[str, Any]:
        """
        ä½¿ç”¨GPT-4Våˆ†æå›¾ç‰‡

        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            prompt: åˆ†ææç¤ºè¯­

        Returns:
            dict: åˆ†æç»“æœ
        """
        if not self.openai_key:
            return {
                'success': False,
                'error': 'OpenAI APIå¯†é’¥æœªé…ç½®'
            }

        # éªŒè¯å›¾ç‰‡
        valid, error = self.validate_image(image_path)
        if not valid:
            return {'success': False, 'error': error}

        # ç¼–ç å›¾ç‰‡
        base64_image = self.encode_image(image_path)

        try:
            # è°ƒç”¨OpenAI API
            url = "https://api.openai.com/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.openai_key}",
                "Content-Type": "application/json"
            }

            data = {
                "model": "gpt-4-vision-preview",
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            }
                        ]
                    }
                ],
                "max_tokens": 1024
            }

            response = requests.post(
                url, headers=headers, json=data, timeout=30)
            response.raise_for_status()

            result = response.json()

            return {
                'success': True,
                'description': result['choices'][0]['message']['content'],
                'model': 'gpt-4-vision',
                'timestamp': datetime.now().isoformat()
            }

        except requests.exceptions.RequestException as e:
            return {
                'success': False,
                'error': f'APIè¯·æ±‚å¤±è´¥: {str(e)}'
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'åˆ†æå¤±è´¥: {str(e)}'
            }

    def analyze_image(self, image_path: str, prompt: Optional[str] = None,
                      prefer_model: str = "auto") -> Dict[str, Any]:
        """
        æ™ºèƒ½å›¾ç‰‡åˆ†æï¼ˆè‡ªåŠ¨é€‰æ‹©å¯ç”¨æ¨¡å‹ï¼‰

        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            prompt: åˆ†ææç¤ºè¯­ï¼ˆå¯é€‰ï¼‰
            prefer_model: ä¼˜å…ˆä½¿ç”¨çš„æ¨¡å‹ ("qwen", "claude", "gpt4v", "auto")

        Returns:
            dict: åˆ†æç»“æœ
        """
        # é»˜è®¤æç¤ºè¯­
        if prompt is None:
            prompt = "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬åœºæ™¯ã€ç‰©ä½“ã€äººç‰©ã€æ–‡å­—ç­‰æ‰€æœ‰å¯è§å…ƒç´ ã€‚"

        # æ£€æŸ¥å¯†é’¥æ˜¯å¦æœ‰æ•ˆ
        valid_qwen = self.qwen_key and self.qwen_key != "your_qwen_api_key_here"
        valid_claude = self.claude_key and len(self.claude_key) > 30
        valid_openai = self.openai_key and self.openai_key != "your_openai_api_key_here"

        # Auto æ¨¡å¼ï¼šä¼˜å…ˆ Qwenï¼ˆå›½å†…å¯ç”¨ï¼‰
        if prefer_model == "auto":
            if valid_qwen:
                result = self.analyze_with_qwen(image_path, prompt)
                if result['success']:
                    return result
                print(f"âš ï¸ Qwenå¤±è´¥: {result.get('error')}")

            if valid_claude:
                result = self.analyze_with_claude(image_path, prompt)
                if result['success']:
                    return result
                print(f"âš ï¸ Claudeå¤±è´¥: {result.get('error')}")

            if valid_openai:
                return self.analyze_with_gpt4v(image_path, prompt)

            return {'success': False, 'error': 'æ²¡æœ‰é…ç½®å¯ç”¨çš„è§†è§‰API (æ¨èé…ç½® QWEN_API_KEY)'}

        # æŒ‡å®šä½¿ç”¨ Qwen
        if prefer_model == "qwen":
            return self.analyze_with_qwen(image_path, prompt)

        # æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©æ¨¡å‹
        if prefer_model == "claude" or (prefer_model == "auto" and self.claude_key):
            result = self.analyze_with_claude(image_path, prompt)
            if result['success']:
                return result
            # Claudeå¤±è´¥ï¼Œå°è¯•GPT-4V
            if self.openai_key:
                return self.analyze_with_gpt4v(image_path, prompt)
            return result

        elif prefer_model == "gpt4v" or (prefer_model == "auto" and self.openai_key):
            result = self.analyze_with_gpt4v(image_path, prompt)
            if result['success']:
                return result
            # GPT-4Vå¤±è´¥ï¼Œå°è¯•Claude
            if self.claude_key:
                return self.analyze_with_claude(image_path, prompt)
            return result

        else:
            return {
                'success': False,
                'error': 'æ²¡æœ‰é…ç½®å¯ç”¨çš„è§†è§‰API (éœ€è¦CLAUDE_API_KEYæˆ–OPENAI_API_KEY)'
            }

    def save_upload(self, file_data: bytes, filename: str) -> tuple[bool, str]:
        """
        ä¿å­˜ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶

        Args:
            file_data: æ–‡ä»¶äºŒè¿›åˆ¶æ•°æ®
            filename: æ–‡ä»¶å

        Returns:
            tuple: (æ˜¯å¦æˆåŠŸ, æ–‡ä»¶è·¯å¾„æˆ–é”™è¯¯ä¿¡æ¯)
        """
        try:
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_filename = f"{timestamp}_{filename}"
            file_path = self.upload_dir / safe_filename

            # ä¿å­˜æ–‡ä»¶
            with open(file_path, 'wb') as f:
                f.write(file_data)

            # è¿”å›ç›¸å¯¹è·¯å¾„ï¼ˆç”¨äºå‰ç«¯è®¿é—®ï¼‰
            relative_path = f"uploads/{safe_filename}"
            return True, relative_path

        except Exception as e:
            return False, f"ä¿å­˜å¤±è´¥: {str(e)}"


# å·¥å…·æ¥å£ï¼ˆä¾›tool_managerè°ƒç”¨ï¼‰
def vision_tool_interface(parameters: Dict[str, Any]) -> Dict[str, Any]:
    """
    è§†è§‰å·¥å…·æ¥å£

    Parameters:
        image_path: å›¾ç‰‡è·¯å¾„ (å¿…éœ€)
        prompt: åˆ†ææç¤ºè¯­ (å¯é€‰)
        model: ä¼˜å…ˆæ¨¡å‹ "claude"/"gpt4v"/"auto" (å¯é€‰ï¼Œé»˜è®¤auto)

    Returns:
        dict: å›¾ç‰‡åˆ†æç»“æœ
    """
    tool = VisionTool()

    # è·å–å‚æ•°
    image_path = parameters.get('image_path')
    prompt = parameters.get('prompt')
    model = parameters.get('model', 'auto')

    if not image_path:
        return {
            'success': False,
            'error': 'ç¼ºå°‘å¿…éœ€å‚æ•°: image_path'
        }

    # åˆ†æå›¾ç‰‡
    return tool.analyze_image(image_path, prompt, model)


# å·¥å…·å…ƒæ•°æ®
VISION_TOOL_META = {
    'name': 'vision',
    'description': 'åˆ†æå’Œç†è§£å›¾ç‰‡å†…å®¹ï¼Œè¯†åˆ«ç‰©ä½“ã€åœºæ™¯ã€æ–‡å­—ç­‰',
    'category': 'multimodal',
    'parameters': {
        'image_path': {
            'type': 'string',
            'description': 'å›¾ç‰‡æ–‡ä»¶è·¯å¾„',
            'required': True
        },
        'prompt': {
            'type': 'string',
            'description': 'åˆ†ææç¤ºè¯­ï¼ˆå¯é€‰ï¼‰',
            'required': False
        },
        'model': {
            'type': 'string',
            'description': 'ä¼˜å…ˆä½¿ç”¨çš„æ¨¡å‹: claude/gpt4v/auto',
            'required': False,
            'default': 'auto'
        }
    },
    'examples': [
        {
            'prompt': 'åˆ†æè¿™å¼ å›¾ç‰‡',
            'parameters': {'image_path': 'uploads/photo.jpg'}
        },
        {
            'prompt': 'å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆæ–‡å­—ï¼Ÿ',
            'parameters': {
                'image_path': 'uploads/document.png',
                'prompt': 'è¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹'
            }
        }
    ]
}


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª æµ‹è¯•Vision Tool")
    print("=" * 60)

    tool = VisionTool()

    # æµ‹è¯•å›¾ç‰‡éªŒè¯
    print("\næµ‹è¯•1: å›¾ç‰‡éªŒè¯")
    valid, error = tool.validate_image("test.jpg")
    print(f"ç»“æœ: {'âœ… æœ‰æ•ˆ' if valid else f'âŒ {error}'}")

    print("\nâœ… Vision Toolåˆå§‹åŒ–æˆåŠŸ")
    print(f"æ”¯æŒæ ¼å¼: {', '.join(tool.supported_formats)}")
    print(f"ä¸Šä¼ ç›®å½•: {tool.upload_dir}")
    print(f"Claudeå¯ç”¨: {'âœ…' if tool.claude_key else 'âŒ'}")
    print(f"GPT-4Vå¯ç”¨: {'âœ…' if tool.openai_key else 'âŒ'}")
