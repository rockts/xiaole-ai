# v0.7.0 实际测试指南

## 📋 测试环境

- 服务地址: http://localhost:8000
- 测试时间: 2025-11-14
- 测试版本: v0.7.0

## 🎯 测试目标

验证以下4项核心功能的实际效果：

1. **智能触发 - 知识空白检测**
2. **智能触发 - 信息冲突检测**
3. **智能触发 - 任务反馈追踪**
4. **情感感知 - 不耐烦检测**

---

## 🧪 测试场景

### 场景1: 知识空白检测

**目标**: 验证AI在回答模糊时，系统能否自动触发追问

**测试步骤**:

1. **提问**: "RAG技术有什么优势？"
2. **预期**: AI给出一个相对简单的回答
3. **观察**: 是否出现`💭`追问标记，内容是否合理

**示例对话**:
```
用户: RAG技术有什么优势？
AI: RAG技术可以检索知识库来增强回答。💭 这个问题可以再详细说说吗？
```

**成功标准**:
- ✅ 检测到回答过短/模糊
- ✅ 自动生成追问
- ✅ 追问内容自然合理

---

### 场景2: 信息冲突检测

**目标**: 验证系统能否检测用户前后矛盾的说法

**测试步骤**:

1. **第一句**: "我喜欢喝咖啡"
   - 观察: AI正常回复，记忆层记录这个偏好

2. **第二句** (1分钟后): "我不喜欢喝咖啡"
   - 观察: 是否触发冲突检测，出现`💭`追问

**示例对话**:
```
用户: 我喜欢喝咖啡
AI: 好的，记住了你的偏好。

[1分钟后]
用户: 我不喜欢喝咖啡
AI: 收到。💭 我记得之前你说过「喜欢喝咖啡」，这次的说法好像不太一样？
```

**成功标准**:
- ✅ 记忆层记录第一次的说法
- ✅ 检测到情感极性冲突
- ✅ 追问时引用旧信息

---

### 场景3: 任务反馈追踪

**目标**: 验证系统能否检测任务完成但用户未反馈的情况

**测试步骤**:

1. **请求任务**: "帮我设置一个明天下午3点的会议提醒"
   - 观察: AI回复"已设置"类消息

2. **换话题** (不给反馈): "今天天气怎么样？"
   - 观察: 是否触发反馈追问

**示例对话**:
```
用户: 帮我设置一个明天下午3点的会议提醒
AI: 已为你设置明天下午3点的会议提醒。

用户: 今天天气怎么样？
AI: 今天天气晴朗。💭 刚才的会议提醒完成了，效果怎么样？
```

**成功标准**:
- ✅ 检测到"已设置"等任务标志词
- ✅ 检测到用户未反馈
- ✅ 自动追问任务效果

---

### 场景4: 情感感知（不耐烦检测）

**目标**: 验证系统能否感知用户不耐烦并停止追问

**测试步骤**:

1. **第一轮**: "Python有什么特点？"
   - AI回复

2. **第二轮**: "就是简单"
   - 观察: 可能触发追问（因为回答太短）

3. **第三轮**: "别问了，知道了"
   - 观察: 应该**不再**触发追问

**示例对话**:
```
用户: Python有什么特点？
AI: Python简洁易读，适合初学者。

用户: 就是简单
AI: 明白了。💭 能具体说说Python的简单体现在哪些方面吗？

用户: 别问了，知道了
AI: 好的，明白了。（不再追问）
```

**成功标准**:
- ✅ 检测到"别问了"等不耐烦标志词
- ✅ 停止追问（没有`💭`）
- ✅ 回复更简短委婉

---

## 📊 测试记录表

| 场景         | 是否触发 | 追问内容 | 质量评分(1-10) | 备注 |
| ------------ | -------- | -------- | -------------- | ---- |
| 知识空白检测 |          |          |                |      |
| 信息冲突检测 |          |          |                |      |
| 任务反馈追踪 |          |          |                |      |
| 情感感知     |          |          |                |      |

---

## 🔍 调试技巧

### 1. 查看服务器日志
```bash
tail -f /tmp/xiaole_server.log
```

关注这些关键词:
- `检测到知识空白`
- `检测到信息冲突`
- `检测到用户不耐烦`
- `智能追问触发`

### 2. 检查数据库记录
```bash
.venv/bin/python -c "
from proactive_qa import ProactiveQA
qa = ProactiveQA()
history = qa.get_followup_history(limit=10)
for h in history:
    print(f'{h[\"type\"]}: {h[\"followup_question\"]}')
"
```

### 3. 调整触发阈值
在`.env`文件中修改:
```env
PROACTIVE_QA_THRESHOLD=65  # 默认65，可调整到50-80
```

---

## 📈 预期性能指标

基于v0.7.0的目标性能：

| 指标           | v0.6.2 | v0.7.0目标 | 实测值 |
| -------------- | ------ | ---------- | ------ |
| 误触发率       | 30%    | ≤15%       |        |
| 冲突检测准确率 | 60%    | ≥85%       |        |
| 追问质量评分   | 6/10   | ≥8/10      |        |
| 情感识别成功率 | 0%     | ≥90%       |        |

---

## ⚠️ 已知限制

1. **语义冲突检测**
   - 当前使用字符级Jaccard相似度
   - 对于"我不太想喝咖啡"这种弱否定可能检测不出
   - 未来可使用embedding提升

2. **情感检测**
   - 仅支持中文标志词
   - 英文或其他语言可能失效

3. **任务反馈**
   - 依赖关键词检测（"已设置"、"完成"等）
   - 复杂表述可能漏检

---

## ✅ 测试完成检查清单

- [ ] 知识空白检测能正常触发
- [ ] 信息冲突能准确识别
- [ ] 任务反馈追踪有效
- [ ] 情感感知能避免过度追问
- [ ] 追问内容自然流畅
- [ ] 没有明显bug或异常
- [ ] 误触发率在可接受范围

---

## 📝 问题反馈

测试中发现问题请记录：

### 问题1: [描述]
- **场景**: 
- **预期**: 
- **实际**: 
- **严重程度**: 低/中/高

### 问题2: [描述]
...

---

## 🎉 测试结论

**整体评价**: ⭐⭐⭐⭐⭐ (1-5星)

**亮点**:
- 

**不足**:
- 

**改进建议**:
- 

---

**测试人员**: _____________  
**测试日期**: 2025-11-14  
**签名**: _____________
