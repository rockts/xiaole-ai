from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine, func, or_
from db_setup import Memory
import os
from dotenv import load_dotenv
from datetime import datetime, timedelta
from semantic_search import SemanticSearchManager

load_dotenv()

# ä¼˜å…ˆä½¿ç”¨ DATABASE_URLï¼Œå¦‚æœæ²¡æœ‰åˆ™æ„å»ºPostgreSQL URL
if os.getenv('DATABASE_URL'):
    DB_URL = os.getenv('DATABASE_URL')
else:
    DB_URL = (
        f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASS')}"
        f"@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}"
        f"/{os.getenv('DB_NAME')}"
    )

engine = create_engine(
    DB_URL,
    connect_args={'check_same_thread': False} if DB_URL.startswith('sqlite')
    else {'client_encoding': 'utf8'}
)
Session = sessionmaker(bind=engine)


class MemoryManager:
    def __init__(self, enable_vector_search=True):  # é»˜è®¤å¯ç”¨è¯­ä¹‰æœç´¢
        self.session = Session()
        self.enable_vector_search = enable_vector_search

        # åˆå§‹åŒ–è¯­ä¹‰æœç´¢ç®¡ç†å™¨
        if self.enable_vector_search:
            try:
                self.semantic_search = SemanticSearchManager()
                print("âœ… è¯­ä¹‰æœç´¢å·²å¯ç”¨")
            except Exception as e:
                print(f"âš ï¸ è¯­ä¹‰æœç´¢åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œé™çº§åˆ°å…³é”®è¯æœç´¢")
                self.enable_vector_search = False
                self.semantic_search = None
        else:
            self.semantic_search = None

    def remember(self, content, tag="general", initial_importance=0.5):
        """
        Store memory with importance score.

        Args:
            content: memory content
            tag: tag/category
            initial_importance: importance score (0-1) - æš‚æ—¶æœªä½¿ç”¨ï¼Œç­‰å¾…æ•°æ®åº“è¿ç§»
        """
        # å»é‡æ£€æŸ¥ï¼šå¦‚æœæ˜¯ facts æ ‡ç­¾ï¼Œæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå†…å®¹
        if tag == "facts":
            existing = self.session.query(Memory).filter(
                Memory.tag == "facts",
                Memory.content == content
            ).first()

            if existing:
                print(f"âš ï¸ è·³è¿‡é‡å¤ facts: {content[:50]}")
                return existing.id

        memory = Memory(
            content=content,
            tag=tag
            # importance_score å­—æ®µéœ€è¦æ•°æ®åº“è¿ç§»åæ‰èƒ½ä½¿ç”¨
        )
        self.session.add(memory)
        self.session.commit()

        # æ·»åŠ åˆ°è¯­ä¹‰æœç´¢ç´¢å¼•
        if self.enable_vector_search and self.semantic_search:
            try:
                self.semantic_search.add_memory(memory.id, content, tag)
            except Exception as e:
                print(f"æ·»åŠ è¯­ä¹‰ç´¢å¼•å¤±è´¥: {e}")

        return memory.id

    def recall(self, tag="general", keyword=None, limit=None):
        """Recall memories by tag and keyword"""
        query = self.session.query(Memory).filter(Memory.tag == tag)

        # å¦‚æœæœ‰å…³é”®è¯ï¼Œè¿›è¡Œæ¨¡ç³Šæœç´¢
        if keyword:
            query = query.filter(Memory.content.contains(keyword))

        # æŒ‰æ—¶é—´å€’åº
        query = query.order_by(Memory.created_at.desc())

        # é™åˆ¶æ•°é‡
        if limit:
            query = query.limit(limit)

        memories = query.all()
        return [m.content for m in memories]

    def recall_recent(self, hours=24, tag=None, limit=10):
        """Recall recent memories within specified hours"""
        time_threshold = datetime.now() - timedelta(hours=hours)

        query = self.session.query(Memory).filter(
            Memory.created_at >= time_threshold
        )

        if tag:
            query = query.filter(Memory.tag == tag)

        query = query.order_by(Memory.created_at.desc()).limit(limit)
        memories = query.all()

        # è¿”å›å®Œæ•´å¯¹è±¡ä¿¡æ¯ï¼Œä¾›å‰ç«¯æ˜¾ç¤º
        return [{
            'id': m.id,
            'content': m.content,
            'tag': m.tag,
            'timestamp': m.created_at.strftime('%Y-%m-%d %H:%M:%S')
        } for m in memories]

    def recall_by_keywords(self, keywords, tag=None, limit=10):
        """Search memories by multiple keywords with OR logic"""
        if not keywords:
            return []

        # æ„å»ºå…³é”®è¯è¿‡æ»¤æ¡ä»¶
        keyword_filters = [
            Memory.content.contains(kw) for kw in keywords
        ]

        query = self.session.query(Memory).filter(or_(*keyword_filters))

        if tag:
            query = query.filter(Memory.tag == tag)

        query = query.order_by(Memory.created_at.desc()).limit(limit)
        memories = query.all()

        # è¿”å›å®Œæ•´çš„è®°å¿†ä¿¡æ¯
        return [{
            'id': m.id,
            'content': m.content,
            'tag': m.tag,
            'timestamp': m.created_at.strftime('%Y-%m-%d %H:%M:%S')
        } for m in memories]

    def get_stats(self):
        """Get memory statistics"""
        # æ€»è®°å¿†æ•°
        total = self.session.query(func.count(Memory.id)).scalar()

        # æŒ‰æ ‡ç­¾ç»Ÿè®¡
        tag_stats = self.session.query(
            Memory.tag,
            func.count(Memory.id)
        ).group_by(Memory.tag).all()

        return {
            "total": total,
            "by_tag": {tag: count for tag, count in tag_stats}
        }

    def semantic_recall(self, query, tag=None, limit=10, min_score=0.15):
        """Semantic search using TF-IDF and cosine similarity"""
        if not self.enable_vector_search or not self.semantic_search:
            # é™çº§åˆ°å…³é”®è¯æœç´¢
            print("âš ï¸ è¯­ä¹‰æœç´¢ä¸å¯ç”¨ï¼Œä½¿ç”¨å…³é”®è¯æœç´¢")
            keywords = query.split()
            return self.recall_by_keywords(keywords, tag=tag, limit=limit)

        try:
            # æŸ¥è¯¢æ‰€æœ‰è®°å¿†
            query_obj = self.session.query(Memory)
            if tag:
                query_obj = query_obj.filter(Memory.tag == tag)

            all_memories = query_obj.all()

            if not all_memories:
                return []

            # æ„å»ºæ–‡æ¡£åˆ—è¡¨ï¼š(id, content)
            documents = [(m.id, m.content) for m in all_memories]

            # æ‰§è¡Œè¯­ä¹‰æœç´¢
            results = self.semantic_search.search(
                query=query,
                documents=documents,
                top_k=limit,
                min_score=min_score
            )

            if not results:
                return []

            # è·å–åŒ¹é…çš„è®°å¿†è¯¦æƒ…
            result_ids = [mem_id for mem_id, _ in results]
            id_to_score = {mem_id: score for mem_id, score in results}

            matched_memories = self.session.query(Memory).filter(
                Memory.id.in_(result_ids)
            ).all()

            # v0.6.0: æ›´æ–°è®¿é—®è®°å½• (éœ€è¦æ•°æ®åº“è¿ç§»åå¯ç”¨)
            # for mem in matched_memories:
            #     self._update_access(mem.id)

            # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›
            memories_with_scores = [
                {
                    'id': m.id,
                    'content': m.content,
                    'tag': m.tag,
                    'timestamp': m.created_at.strftime('%Y-%m-%d %H:%M:%S'),
                    'score': id_to_score.get(m.id, 0)
                }
                for m in matched_memories
            ]

            # æŒ‰åˆ†æ•°é™åºæ’åº
            memories_with_scores.sort(key=lambda x: x['score'], reverse=True)

            return memories_with_scores

        except Exception as e:
            print(f"âš ï¸ è¯­ä¹‰æœç´¢å¤±è´¥: {e}ï¼Œé™çº§åˆ°å…³é”®è¯æœç´¢")
            keywords = query.split()
            return self.recall_by_keywords(keywords, tag=tag, limit=limit)

    # v0.6.0 Phase 3æ–¹æ³•: éœ€è¦æ•°æ®åº“è¿ç§»åå¯ç”¨
    # def _update_access(self, memory_id):
    #     """æ›´æ–°è®°å¿†è®¿é—®è®°å½•"""
    #     mem = self.session.query(Memory).filter(
    #         Memory.id == memory_id
    #     ).first()
    #     if mem:
    #         mem.access_count = (mem.access_count or 0) + 1
    #         mem.last_accessed_at = datetime.now()
    #         self.session.commit()
    #
    # def calculate_importance(self, memory_id):
    #     """è®¡ç®—è®°å¿†çš„é‡è¦æ€§åˆ†æ•°"""
    #     ...
    #
    # def update_all_importance_scores(self):
    #     """æ‰¹é‡æ›´æ–°æ‰€æœ‰è®°å¿†çš„é‡è¦æ€§åˆ†æ•°"""
    #     ...
    #
    # def get_top_memories(self, limit=20, tag=None):
    #     """è·å–æœ€é‡è¦çš„è®°å¿†"""
    #     ...
    #
    # def auto_archive_low_importance(self, threshold=0.1, min_age_days=30):
    #     """è‡ªåŠ¨å½’æ¡£ä½é‡è¦æ€§è®°å¿†"""
    #     ...

    def calculate_importance(self, memory_id):
        """
        v0.6.0: è®¡ç®—è®°å¿†çš„é‡è¦æ€§åˆ†æ•°

        è€ƒè™‘å› ç´ :
        1. è®¿é—®é¢‘ç‡ (40 %)
        2. æ—¶é—´è¡°å‡ (30 %)
        3. æ ‡ç­¾ç±»å‹ (30 %)

        Returns:
            float: é‡è¦æ€§åˆ†æ•°(0-1)
        """
        mem = self.session.query(Memory).filter(
            Memory.id == memory_id
        ).first()

        if not mem:
            return 0.0

        # 1. è®¿é—®é¢‘ç‡åˆ†æ•° (0-1)
        # è®¿é—®æ¬¡æ•°è¶Šå¤šè¶Šé‡è¦ï¼Œä½¿ç”¨å¯¹æ•°å‡½æ•°é¿å…çº¿æ€§å¢é•¿
        import math
        access_score = min(
            math.log(mem.access_count + 1) / math.log(100),
            1.0
        )

        # 2. æ—¶é—´è¡°å‡åˆ†æ•° (0-1)
        # æœ€è¿‘çš„è®°å¿†æ›´é‡è¦ï¼Œä½¿ç”¨æŒ‡æ•°è¡°å‡
        days_ago = (datetime.now() - mem.created_at).days
        time_score = math.exp(-days_ago / 30.0)  # 30å¤©åŠè¡°æœŸ

        # 3. æ ‡ç­¾æƒé‡ (0-1)
        tag_weights = {
            'facts': 1.0,      # ç”¨æˆ·æ˜ç¡®å‘ŠçŸ¥çš„äº‹å®æœ€é‡è¦
            'task': 0.8,       # ä»»åŠ¡è®°å½•æ¬¡é‡è¦
            'general': 0.5,    # æ™®é€šå¯¹è¯ä¸­ç­‰é‡è¦
            'system': 0.3      # ç³»ç»Ÿæ—¥å¿—è¾ƒä¸é‡è¦
        }
        tag_score = tag_weights.get(mem.tag, 0.5)

        # Weighted average calculation
        importance = (
            access_score * 0.4 +
            time_score * 0.3 +
            tag_score * 0.3
        )

        # æ›´æ–°æ•°æ®åº“
        mem.importance_score = importance
        self.session.commit()

        return importance

    def update_all_importance_scores(self):
        """
        v0.6.0: æ‰¹é‡æ›´æ–°æ‰€æœ‰è®°å¿†çš„é‡è¦æ€§åˆ†æ•°

        Returns:
            int: æ›´æ–°çš„è®°å¿†æ•°é‡
        """
        all_memories = self.session.query(Memory).filter(
            Memory.is_archived == False  # noqa: E712
        ).all()

        updated_count = 0
        for mem in all_memories:
            self.calculate_importance(mem.id)
            updated_count += 1

        print(f"âœ… å·²æ›´æ–° {updated_count} æ¡è®°å¿†çš„é‡è¦æ€§åˆ†æ•°")
        return updated_count

    def get_top_memories(self, limit=20, tag=None):
        """
        v0.6.0: è·å–æœ€é‡è¦çš„è®°å¿†

        Args:
            limit: è¿”å›æ•°é‡
            tag: å¯é€‰æ ‡ç­¾è¿‡æ»¤

        Returns:
            [{content, tag, importance_score, access_count}]
        """
        query = self.session.query(Memory).filter(
            Memory.is_archived == False  # noqa: E712
        )

        if tag:
            query = query.filter(Memory.tag == tag)

        query = query.order_by(
            Memory.importance_score.desc()
        ).limit(limit)

        memories = query.all()

        return [{
            'content': m.content,
            'tag': m.tag,
            'importance_score': m.importance_score,
            'access_count': m.access_count,
            'created_at': m.created_at.strftime('%Y-%m-%d %H:%M:%S')
        } for m in memories]

    def auto_archive_low_importance(self, threshold=0.1, min_age_days=30):
        """
        v0.6.0: è‡ªåŠ¨å½’æ¡£ä½é‡è¦æ€§è®°å¿†

        å½’æ¡£ç­–ç•¥:
        - é‡è¦æ€§åˆ†æ•° < threshold
        - åˆ›å»ºæ—¶é—´ > min_age_days å¤©
        - è®¿é—®æ¬¡æ•° <= 1

        Args:
            threshold: importance threshold (0-1)
            min_age_days: minimum age in days

        Returns:
            int: archived count
        """
        from datetime import timedelta

        cutoff_date = datetime.now() - timedelta(days=min_age_days)

        # æŸ¥æ‰¾éœ€è¦å½’æ¡£çš„è®°å¿†
        low_importance_memories = self.session.query(Memory).filter(
            Memory.is_archived == False,  # noqa: E712
            Memory.importance_score < threshold,
            Memory.created_at < cutoff_date,
            Memory.access_count <= 1
        ).all()

        archived_count = 0
        for mem in low_importance_memories:
            mem.is_archived = True
            archived_count += 1

        self.session.commit()

        if archived_count > 0:
            print(f"âœ… å·²å½’æ¡£ {archived_count} æ¡ä½é‡è¦æ€§è®°å¿†")

        return archived_count

    def get_memory_stats(self):
        """Get memory statistics with importance analysis"""
        # åŸºç¡€ç»Ÿè®¡
        total = self.session.query(func.count(Memory.id)).scalar()
        archived = self.session.query(func.count(Memory.id)).filter(
            Memory.is_archived == True  # noqa: E712
        ).scalar()
        active = total - archived

        # æŒ‰æ ‡ç­¾ç»Ÿè®¡
        tag_stats = self.session.query(
            Memory.tag,
            func.count(Memory.id)
        ).filter(
            Memory.is_archived == False  # noqa: E712
        ).group_by(Memory.tag).all()

        # é‡è¦æ€§åˆ†å¸ƒ
        high_importance = self.session.query(func.count(Memory.id)).filter(
            Memory.is_archived == False,  # noqa: E712
            Memory.importance_score >= 0.7
        ).scalar()

        medium_importance = self.session.query(func.count(Memory.id)).filter(
            Memory.is_archived == False,  # noqa: E712
            Memory.importance_score >= 0.3,
            Memory.importance_score < 0.7
        ).scalar()

        low_importance = self.session.query(func.count(Memory.id)).filter(
            Memory.is_archived == False,  # noqa: E712
            Memory.importance_score < 0.3
        ).scalar()

        return {
            "total": total,
            "active": active,
            "archived": archived,
            "by_tag": {tag: count for tag, count in tag_stats},
            "importance_distribution": {
                "high (â‰¥0.7)": high_importance,
                "medium (0.3-0.7)": medium_importance,
                "low (<0.3)": low_importance
            }
        }

    def cleanup_old_conversations(self, days=7):
        """
        æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„ conversation è®°å¿†

        Args:
            days: ä¿ç•™å¤©æ•°ï¼Œé»˜è®¤7å¤©

        Returns:
            åˆ é™¤çš„è®°å¿†æ•°é‡
        """
        cutoff_date = datetime.now() - timedelta(days=days)

        old_conversations = self.session.query(Memory).filter(
            Memory.tag.like('conversation:%'),
            Memory.created_at < cutoff_date
        ).all()

        count = len(old_conversations)
        for mem in old_conversations:
            self.session.delete(mem)

        self.session.commit()
        print(f"ğŸ—‘ï¸ æ¸…ç†äº† {count} æ¡è¶…è¿‡{days}å¤©çš„conversationè®°å¿†")
        return count
